---
title: "Day 4 - How much are we Learning?"
#author: "JP"
#date: "January 22, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We look beyond each argument’s own claims to critique the generalizability of the findings, the sensitivity of the findings to the research design, the match between theory and evidence, and support for specific causal mechanisms. We also discuss publication bias, 'p-hacking' and pre-registration.

## Morning

1. [Lecture Slides](https://jonnyphillips.github.io/Causal_Critiques_2020/Day 4/Slides_Day_4_2024_v1.pdf)
2. [Exercises in Generalizability](https://jonnyphillips.github.io/Causal_Critiques_2020/Day 4/Exercises_on_Generalizability.html)
3. [Exercise on Policy Interpretation](https://jonnyphillips.github.io/Causal_Critiques_2020/Day 4/Exercises_Policy.html)

*Readings:*

- Cartwright, Nancy (2007), Are RCTs the Gold Standard?, BioSocieties, 2, 11-20
- Angus S Deaton. Instruments of Development, Randomization in the Tropics and the Search for the Elusive Keys to Economic Development. 2009.
- Steven D Levitt and John A List. What Do Laboratory Experiments Tell Us About the Real World? 2006.
- Hainmueller, Jens et al (2014), "Validating vignette and conjoint survey experiments against real-world behavior", Proceedings of the National Academy of Sciences, 112:8
- Dunning, Thad (2012), Natural Experiments in the Social Sciences: A Design-Based Approach, Ch. 10
- [optional] Monogan, James (2015),  "Research Preregistration in Political Science: The Case, Counterarguments, and a Response to Critiques"
- [optional] Jasjeet S Sekhon and Rocio Titiunik. When Natural Experiments Are Neither Natural nor Experiments. American Political Science Review, 106(1):35-57, 2012.


## Afternoon - Critiquing the Data

How much do we learn from the regression discontinuity results estimated in Day 3? 

First, we will implement some robustness checks:  

1. Our regression discontinuity assumed a *linear* relationship between winning margin and subsequent electoral performance. Implement an alternative regression discontinuity with a more flexible quadratic relationship. How do the results differ?  
2. An alternative way to implement the regression discontinuity is to limit the data to only those units 'close' to the discontinuity. Remove any data that is more than +/-5\% from the threshold and run your (linear) regression discontinuity again. How do the results change?  
3. Estimating the uncertainty (standard errors) for regression discontinuities is tricky. The 'correct' way to do it has been programmed in the 'rdrobust' package. Implement the (linear) regression discontinuity in rdrobust and compare the standard errors.   
4. If our theory is correct, we should only find an effect at the threshold (distance=0). Use the ‘RDestimate’ function (in the ‘rdd’ library) to perform placebo tests with the threshold set to distance=0.1,-0.1,0.05 and -0.05. What do we learn from this test?

What can we learn about the scope of our conclusions?

5. To assess how different units near the threshold might be, let's compare which parties are most represented near to the threshold (+/-5\%) with those that win or lose by a landslide (win or lose by more than 20\%).  
6. Based on the above test, how generalizable do you believe the findings in Titiunik 2011 to be?  

*Extra task:* We can further explore the robustness and generalizability of our findings by replicating the study with different data samples. Use the data for the 2012 and 2016 elections to implement a new (linear) regression discontinuity. Is the relationship the same as the results you estimated for 2000-04?
