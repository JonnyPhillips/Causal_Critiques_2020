---
title: "Day 3 - Assessing Causal Evidence"
#author: "JP"
#date: "January 22, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We review a range of causal methodology research designs, the assumptions on which they are based and their connection to specific statistical models. We practice repeatedly assessing if these assumptions have been met.

## Morning

1. [Lecture Slides](https://jonnyphillips.github.io/Causal_Critiques_2020/Day 3/Slides_Day_3_2024_v1.pdf)
2. [Exercises on Causal Methods](https://jonnyphillips.github.io/Causal_Critiques_2020/Day 3/Exercises_on_Causal_Methods.html)

*Readings:* 

- Alan S. Gerber and Donald P Green. Field Experiments: Design, Analysis and Interpretation.
W.W. Norton & Company, 2012. Ch. 1
- Dunning, Thad (2012), Natural Experiments in the Social Sciences: A Design-Based Approach, Chs. 3, 4, 8 & 9
- Brady, Henry E. (2004), Data-Set Observations versus Causal-Process Observations: The 2000 U.S. Presidential Election
- [optional] Andrew Eggers, Olle Folke, Anthony Fowler, Jens Hainmueller, Andrew B Hall, and James M Snyder (2013). On the Validity of the Regression Discontinuity Design for Estimating Electoral Effects : New Evidence from Over 40,000 Close Races.

## Afternoon - Practicing Critiques

We work in teams to critique papers' assumptions, and the evidence to support those assumptions. For each of the papers below:

(a) Identify what type of methodology/research design the author is using.
(b) List the assumptions that are required for the methodology to produce valid causal estimates in this context.
(b) Write a critique of the methodology, highlighting whether there is any evidence that the assumptions are met. 
(c) Suggest any alternative explanations which might also be consistent with the research findings.
(d) On a scale of 0-10, how much do you believe the findings of the paper?

1. De La O 2013
2. Kane et al 2004
3. Carnegie and Marinov 2017

## Afternoon - Critiquing the Data

To overcome the causal problems we saw yesterday, Titiunik implements a regression discontinuity.  

1. Implement the regression discontinuity using your measure of 'close elections' in 2000, the indicator of incumbency status in 2000 and the measure of electoral performance in 2004.  
2. Interpret the findings of the regression discontinuity. How do they differ from the observational results in Day 1?  

Now let's go back and check our assumptions for the regression discontinuity design to see if we should trust these new results:  

3. One assumption of our regression discontinuity is that comparing incumbents that just won and lost elections in 2000 will produce 'balance' on potential omitted variables. There are thousands of variables we could check, but let's assess balance on the size of the municipality by comparing the number of voters in 2000 within 5\% of a tied election.  
4. How does the balance close to the threshold compare with the balance of winners and losers in the full dataset?  
5. Check for balance within 5\% of the threshold on other characteristics of the municipalities between treated and control units using the variables population, IDHM (Human Development Index) and PIB (GDP per capita).
6. Another assumption of regression discontinuity is that parties cannot manipulate their 'score' on the running variable - their vote share. We can assess this by checking for continuity in the distribution of the variable measuring distance to the threshold, in our case winning margin in 2000. Test this assumption by implementing a McCrary density test (DCdensity in the ‘rdd’ library). What do the results show?
